<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Eleuther AI site</title>
    <link>https://eleutherai.github.io/</link>
    <description>Recent content in Home on Eleuther AI site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Apr 2019 20:18:54 +0300</lastBuildDate><atom:link href="https://eleutherai.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Welcome to EleutherAI</title>
      <link>https://eleutherai.github.io/blog/welcome-to-eleutherai/</link>
      <pubDate>Fri, 08 Jan 2021 11:07:40 +0100</pubDate>
      
      <guid>https://eleutherai.github.io/blog/welcome-to-eleutherai/</guid>
      <description>Welcome to eleutherAI. We are still working on a website so there is no content on this blog yet.
Stay tuned though&amp;hellip; exciting things are coming.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://eleutherai.github.io/about/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/about/</guid>
      <description>About Us EleutherAI (/iˈluθər eɪ. aɪ/) is a grassroots collection of researchers working to open source AI research. Founded in July of 2020, our flagship project is GPT-Neo, a replication of OpenAI&amp;rsquo;s massive 175B parameter language model, GPT-3. Our Discord server is open and welcomes contributors of all backgrounds.</description>
    </item>
    
    <item>
      <title>Get Involved</title>
      <link>https://eleutherai.github.io/get-involved/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/get-involved/</guid>
      <description>Get Involved We welcome contributors of all backgrounds and experience levels. Joining EleutherAI is as simple as joining us on Discord and picking a project to contribute to. However we have some areas where we are particularly interested in recruiting experienced collaborators:
  GPT-Neo is looking for people with expertise in ML pipelining and developing ML apps with user-friendly UIs.
  The Pile is looking for people with access to high-quality data or the ability to scrape, clean, and process large-scale data sources.</description>
    </item>
    
    <item>
      <title>Gpt-Neo</title>
      <link>https://eleutherai.github.io/projects/gpt-neo/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/projects/gpt-neo/</guid>
      <description>GPT-Neo GPT-Neo is the code name for a series of transformer-based language models loosely styled around the GPT architecture that we plan to train and open source. Our primary goal is to replicate a GPT-3 sized model and open source it to the public, for free.
Along the way we will be running experiments with alternative architectures and attention types, releasing any intermediate models, and writing up any findings on our blog.</description>
    </item>
    
    <item>
      <title>Open Web Text 2</title>
      <link>https://eleutherai.github.io/projects-intros/open-web-text2/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/projects-intros/open-web-text2/</guid>
      <description>Open Web Text 2 The core principle of WebText is to build a high-quality internet dataset by extracting URLs from Reddit submissions, scraping the URLs, and then performing filtering for quality (by upvotes) &amp;amp; deduplication. As the dataset collected for training the original GPT-2 is not public, researchers independently reproduced the pipeline and released the resulting dataset, called OpenWebTextCorpus (OWT).
OpenWebText2 (OWT2) is an enhanced version of the original OpenWebTextCorpus covering all Reddit submissions from 2005 up until April 2020, with further months becoming available after the corresponding PushShift dump files are released.</description>
    </item>
    
    <item>
      <title>Open Web Text 2</title>
      <link>https://eleutherai.github.io/projects/open-web-text2/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/projects/open-web-text2/</guid>
      <description>Open Web Text 2 WebText is an internet dataset created by extracting URLs from Reddit submissions and scraping the URLs. It was collected for training the original GPT-2 and never released to the public, researchers independently reproduced the pipeline and released the resulting dataset, called OpenWebTextCorpus (OWT).
OpenWebText2 (OWT2) is an enhanced version of the original OpenWebTextCorpus covering all Reddit submissions from 2005 up until April 2020, with further months becoming available after the corresponding PushShift dump files are released.</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://eleutherai.github.io/publications/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/publications/</guid>
      <description>Publications Preprints Under Review: Komatsuzaki. &amp;ldquo;Current Limitations of Language Models: What You Need is Retrieval.&amp;rdquo; [ arXiv ]
Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy. &amp;ldquo;The Pile: An 800GB Dataset of Diverse Text for Language Modeling.&amp;rdquo; [ arxiv ]</description>
    </item>
    
    <item>
      <title>The Pile</title>
      <link>https://eleutherai.github.io/projects/pile/</link>
      <pubDate>Fri, 26 Apr 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/projects/pile/</guid>
      <description>The Pile The Pile is a large, diverse, open source language modelling data set that consists of many smaller datasets combined together. The objective is to obtain text from as many modalities as possible to ensure that models trained using The Pile will have much broader generalization abilities.  The Pile is now live! Download now, or you can read the docs</description>
    </item>
    
    <item>
      <title>The Pile</title>
      <link>https://eleutherai.github.io/projects-intros/the-pile/</link>
      <pubDate>Tue, 26 Mar 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/projects-intros/the-pile/</guid>
      <description>The Pile The Pile is a large, diverse, open source language modelling data set that consists of many smaller datasets combined together. The objective is to obtain text from as many modalities as possible to ensure that models trained using The Pile will have much broader generalization abilities.
The Pile is now complete! Check it out here.</description>
    </item>
    
    <item>
      <title>Gpt-Neo</title>
      <link>https://eleutherai.github.io/projects-intros/gpt-neo/</link>
      <pubDate>Tue, 26 Feb 2019 20:18:54 +0300</pubDate>
      
      <guid>https://eleutherai.github.io/projects-intros/gpt-neo/</guid>
      <description>GPT-Neo GPT-Neo is the name of our codebase for transformer-based language models loosely styled around the GPT architecture. One of our goals is to use GPT-Neo to replicate a GPT-3 sized model and open source it to the public, for free. Along the way we will be running experiments with alternative architectures and attention types, releasing any intermediate models, and writing up any findings on our blog. Our models are built in Mesh TensorFlow, which will allow us to scale up to GPT-3 sizes and beyond using simultaneous model and data parallelism.</description>
    </item>
    
  </channel>
</rss>
